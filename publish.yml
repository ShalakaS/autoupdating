# Call this whatever you want
name: CPCB_AQI_scrape

# When does it get run?
on:
  # workflow_dispatch means "I can click a button and force it to run"
  workflow_dispatch:
  # schedule/cron means "on a set schedule"
  schedule:
    - cron: '30 12 * * *'
jobs:
  scrape:
    # For some reason, we run love versions of linux
    runs-on: ubuntu-latest
    env:
      GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
    steps:
      # Download all of the code from your repo
      - name: Check out this repo
        uses: actions/checkout@v2
      # Set up Python 3.11
      - name: Set up Python 3.11
        uses: actions/setup-python@v2
        with:
          python-version: '3.11'
      - name: Install necessary Python packages
        run: pip install pandas jupyter lxml requests pdfplumber certifi folium googlemaps pytz python-dotenv openpyxl
      # MAKE SURE YOUR SCRAPER FILENAME MATCHES THE FILENAME HERE!!
      - name: Run the scraping script
        run: jupyter nbconvert --to notebook --execute "aqi_tracker_all_cities.ipynb" --output executed_notebook.ipynb #--stdout
      # I just stole the rest of this code so don't ask me questions about it
      - name: Commit and push if content changed
        run: |-
          git config user.name "Automated"
          git config user.email "actions@users.noreply.github.com"
          git add -A
          timestamp=$(date -u)
          git commit -m "Latest data: ${timestamp}" || exit 0
          git push